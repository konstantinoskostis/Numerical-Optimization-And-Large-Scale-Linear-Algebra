{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29e12c29-53d3-4137-a8b7-2657cf082251",
   "metadata": {},
   "source": [
    "# Numerical Optimization & Large Scale Linear Algebra\n",
    "## Assignment 3 - PageRanking\n",
    "\n",
    "---\n",
    "> Kostis Konstantinos (p3352311) <br/>\n",
    "> Athens University Of Economics And Business <br/>\n",
    "> MSc Data Science - Part Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbe1761-fbd6-4a51-a900-195f2b973a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix, csc_matrix\n",
    "from scipy.sparse.linalg import splu\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9a8d8b-d867-4410-85a2-9072607b5229",
   "metadata": {},
   "source": [
    "## Loading the graph data and creating the P matrix\n",
    "\n",
    "In this section the web pages of standford are loaded from the zip file. \\\n",
    "Then the P matrix (probability transition matrix) is constructed\n",
    "appropriately using the sparse matrix technique as it was indicated in the lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86157d88-f24b-430f-9e26-1f45ca3ccc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('stanweb.dat/stanweb.dat', names = ['source', 'target', 'transition_probability'], sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c600f2b-dcca-4edc-82c8-92add9e91181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct P as a sparse representation\n",
    "def create_sparse_graph(df):\n",
    "    df['source'] = df['source'].astype(int)\n",
    "    df['target'] = df['target'].astype(int)\n",
    "\n",
    "    n = df.source.max()\n",
    "\n",
    "    # Python starts counting from zero\n",
    "    row = df.source - 1\n",
    "    column = df.target - 1\n",
    "    propabilities = df.transition_probability\n",
    "\n",
    "    p_mat = csr_matrix((propabilities, (row, column)), shape=(n, n))\n",
    "\n",
    "    return p_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59546a76-904a-4725-a32d-70ae88c50f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = create_sparse_graph(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5326d82b-5e0b-4d0b-9008-d3769c8605d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shape of the matrix\n",
    "P.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c90c8d-5cfe-41bd-8f26-712c879a53e0",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f36f53-041d-4956-aab2-45edf368b737",
   "metadata": {},
   "source": [
    "## a. Find the pageranking vector $\\pi$\n",
    "\n",
    "Firstly a set of classes is created as needed abstracttions.\\\n",
    "One for the Power-Method technique and one for the system-of-equations using the Gauss-Seidel. \\\n",
    "Each class contains attributes for recording the ranking vector and the time it took to run the specified algorithm.\n",
    "\n",
    "The setup of parameters involves:\n",
    "\n",
    "- $\\alpha = 0.85$\n",
    "- $τ = 10^{−8}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a630d05-8350-4955-a197-a7e27cb1ffeb",
   "metadata": {},
   "source": [
    "### i) PageRanking using the Power-Method\n",
    "\n",
    "The implementation refers in the given PDF described in the formula (1) of the section 5.1 of the given PDF and is essentially:\n",
    "\n",
    "$x^{(k)T} = \\alpha x^{(k-1)T} P + (\\alpha x^{(k-1)T} a + (1 - \\alpha))v^T $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c274a17-0d50-4155-8eae-57c4d6b9dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerMethodPageRank:\n",
    "    def __init__(self, alpha = 0.85, tol=1e-8, persist_topk=None, max_iter=2000):\n",
    "        self.alpha = alpha\n",
    "        self.tol = tol\n",
    "\n",
    "        self.identifier = \"power-method-{}\".format(self.alpha)\n",
    "        self.method_name = \"Power-Method\"\n",
    "\n",
    "        # A max iterations is set by default to 2000\n",
    "        # in case the ranking does not converge (due to some\n",
    "        # numerical instability or glitch)\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        # The following are needed for question c (for the convergence of the components)\n",
    "        self.persist_topk = persist_topk\n",
    "        self.top_nodes = []\n",
    "        self.bottom_nodes = []\n",
    "\n",
    "        # The pageranking vector\n",
    "        self.x = None\n",
    "        \n",
    "        # The running time measured in milliseconds\n",
    "        self.runtime_ms = 0.0\n",
    "\n",
    "        # The number of iterations until convergence\n",
    "        self.n_iterations = 0\n",
    "\n",
    "        # The error history (error per iteration)\n",
    "        self.error_history = []\n",
    "\n",
    "    def updated_ranking(self, G, alpha, x_prev, a, vT):\n",
    "        \"\"\"\n",
    "        This is the formula (1) of the section 5.1 of the given PDF file DeeperInsidePR.\n",
    "        It is the new pageranking vector.\n",
    "        \"\"\"\n",
    "\n",
    "        # Important: The * is used for all types of multiplications.\n",
    "        # This is because if np.dot is used for matrix/vector and vector/vector\n",
    "        # multplications, then the computation blows up because scipy tries\n",
    "        # to unroll the sparse matrix into a non-sparse and Jupyter crashes.\n",
    "        return alpha*x_prev*G + (alpha*x_prev*a + (1 - alpha)) * vT\n",
    "\n",
    "    def fit(self, G):\n",
    "        \"\"\"\n",
    "        Run the pageranking algorithm on graph G\n",
    "        using the Power-Method technique.\n",
    "        \"\"\"\n",
    "\n",
    "        # Setup start time\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        n_r, n_c = G.shape\n",
    "\n",
    "        # Find nodes with no out links\n",
    "        with_no_outlinks = G.sum(axis=1)==0\n",
    "        no_outlinks_index = np.argwhere(with_no_outlinks)\n",
    "\n",
    "        # The a vector having 1 if it is corresponds to a node with no out links and 0 otherwise \n",
    "        a = np.zeros(n_r)\n",
    "        a[no_outlinks_index[:,0]] = 1\n",
    "\n",
    "        # Initial ranking (equi-probable)\n",
    "        x_prev = np.ones(n_c) / n_c\n",
    "\n",
    "        # This could be a personalized vector, but for simplicity it is equi-probable as well.\n",
    "        vT = np.ones(n_c) / n_c\n",
    "\n",
    "        # If instructed, persist the top and bottom nodes ranking\n",
    "        self.persist_top_and_bottom_nodes(x_prev)\n",
    "\n",
    "        while True:\n",
    "            self.n_iterations += 1\n",
    "\n",
    "            # This is the formula (1) of the section 5.1 of the given PDF file DeeperInsidePR.\n",
    "            # It is the new pageranking\n",
    "            x_k = self.updated_ranking(G, self.alpha, x_prev, a, vT)\n",
    "\n",
    "            self.persist_top_and_bottom_nodes(x_k)\n",
    "\n",
    "            e = np.linalg.norm(x_k - x_prev, ord=1)\n",
    "            self.error_history.append(e)\n",
    "\n",
    "            if (self.n_iterations >= self.max_iter) or (e <= self.tol):\n",
    "                break\n",
    "\n",
    "            x_prev = x_k\n",
    "\n",
    "        self.x = x_k\n",
    "\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        self.runtime_ms = (end_time - start_time) * 1000\n",
    "\n",
    "        return self\n",
    "\n",
    "    def sorted_indices(self, ranking=None):\n",
    "        \"\"\"The ranking of the webpages\"\"\"\n",
    "        if ranking is None:\n",
    "            ranking = self.x\n",
    "\n",
    "        ascending_indices = ranking.argsort()\n",
    "        descending_indices = ascending_indices[::-1]\n",
    "\n",
    "        return descending_indices\n",
    "\n",
    "    def persist_top_and_bottom_nodes(self, ranking):\n",
    "        \"\"\"\n",
    "        Persist the ranking of the top-k and bottom-k for\n",
    "        speed of convergennce of the pageranking components.\n",
    "        \"\"\"\n",
    "        if self.persist_topk is not None:\n",
    "            top_k = self.sorted_indices(ranking)[:self.persist_topk]\n",
    "            bottom_k = self.sorted_indices(ranking)[-self.persist_topk:]\n",
    "    \n",
    "            self.top_nodes.append(top_k)\n",
    "            self.bottom_nodes.append(bottom_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae9d265-8f4d-4b5e-9ba6-8b61a1e3fa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the power-method\n",
    "power_method_85 = PowerMethodPageRank(alpha = 0.85, tol=1e-8).fit(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcf96fc-df2d-4b37-9f6d-fe7b006c26fb",
   "metadata": {},
   "source": [
    "### ii) PageRanking by solving the corresponding system (via Gauss-Seidel)\n",
    "\n",
    "The algorithm of Gauss-Seidel that is implemented is based on:\n",
    "\n",
    "- [Gauss Seidel using matrices](https://en.wikipedia.org/wiki/Gauss%E2%80%93Seidel_method)\n",
    "- The formula (3) of the section 5.2 of the given PDF denoting that the pagerank problem using a system of equations\n",
    "  is actually formulated as $\\pi^T (I - \\alpha P) = v^T$\n",
    "\n",
    "If you look closely the last equation is essentially the known $Ax=b$ system of equations. The **LU** method is used and the system\n",
    "solution is found iteratively via the rule:\n",
    "\n",
    "$L x^{k+1} = b - Ux^{k}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae36266-7894-4522-9b7d-8769be027936",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussSeidelPageRank:\n",
    "    def __init__(self, alpha = 0.85, tol=1e-8, persist_topk=None, max_iter=2000):\n",
    "        self.alpha = alpha\n",
    "        self.tol = tol\n",
    "\n",
    "        self.identifier = \"gauss-siedel-{}\".format(self.alpha)\n",
    "        self.method_name = \"System Formulation (Gauss Seidel)\"\n",
    "\n",
    "        # A max iterations is set by default to 2000\n",
    "        # in case the ranking does not converge (due to some\n",
    "        # numerical instability or glitch)\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        # The following are needed for question c (for the convergence of the components)\n",
    "        self.persist_topk = persist_topk\n",
    "        self.top_nodes = []\n",
    "        self.bottom_nodes = []\n",
    "\n",
    "        # The pageranking vector\n",
    "        self.x = None\n",
    "        \n",
    "        # The running time measured in milliseconds\n",
    "        self.runtime_ms = 0.0\n",
    "\n",
    "        # The number of iterations until convergence\n",
    "        self.n_iterations = 0\n",
    "\n",
    "        # The error history (error per iteration)\n",
    "        self.error_history = []\n",
    "\n",
    "    def updated_ranking(self, L, U, x_prev, b):\n",
    "        \"\"\"\n",
    "        This is derived from the formula (3) of the section 5.2 of the given PDF file DeeperInsidePR,\n",
    "        via the LU analysis. It is the new pageranking vector.\n",
    "        \"\"\"\n",
    "\n",
    "        # Important: The * is used for all types of multiplications.\n",
    "        # This is because if np.dot is used for matrix/vector and vector/vector\n",
    "        # multplications, then the computation blows up because scipy tries\n",
    "        # to unroll the sparse matrix into a non-sparse and Jupyter crashes.\n",
    "\n",
    "        return L.solve(b - U*x_prev)\n",
    "\n",
    "    def fit(self, G):\n",
    "        \"\"\"\n",
    "        Run the pageranking algorithm on graph G using the iterative Gauss-Seidel system solution technique.\n",
    "        \"\"\"\n",
    "\n",
    "        # Setup start time\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        n_r, n_c = G.shape\n",
    "\n",
    "        # Construct the identitu matrix\n",
    "        I = sparse.identity(n_r,  format = 'csc')\n",
    "\n",
    "        # Construct A matrix\n",
    "        A = (I - self.alpha * G)\n",
    "\n",
    "        # Decompose A using LU\n",
    "        L = splu(sparse.tril(A, 0,  format = 'csc'))\n",
    "        U = sparse.triu(A, 1,  format = 'csc')\n",
    "\n",
    "        # Initial ranking (equi-probable)\n",
    "        x_prev = np.ones(n_c) / n_c\n",
    "\n",
    "        # This could be a personalized vector, but for simplicity it is equi-probable as well.\n",
    "        b = np.ones(n_c) / n_c\n",
    "\n",
    "        # If instructed, persist the top and bottom nodes ranking\n",
    "        self.persist_top_and_bottom_nodes(x_prev)\n",
    "\n",
    "        while True:\n",
    "            self.n_iterations += 1\n",
    "\n",
    "            x_k = self.updated_ranking(L, U, x_prev, b)\n",
    "\n",
    "            self.persist_top_and_bottom_nodes(x_k)\n",
    "\n",
    "            e = np.linalg.norm(x_k - x_prev, ord=1)\n",
    "            self.error_history.append(e)\n",
    "                \n",
    "\n",
    "            if (self.n_iterations >= self.max_iter) or (e <= self.tol):\n",
    "                break\n",
    "\n",
    "            x_prev = x_k\n",
    "\n",
    "        self.x = x_k\n",
    "\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        self.runtime_ms = (end_time - start_time) * 1000\n",
    "\n",
    "        return self\n",
    "\n",
    "    def sorted_indices(self, ranking=None):\n",
    "        \"\"\"The ranking of the webpages\"\"\"\n",
    "        if ranking is None:\n",
    "            ranking = self.x\n",
    "\n",
    "        ascending_indices = ranking.argsort()\n",
    "        descending_indices = ascending_indices[::-1]\n",
    "\n",
    "        return descending_indices\n",
    "\n",
    "    def persist_top_and_bottom_nodes(self, ranking):\n",
    "        \"\"\"\n",
    "        Persist the ranking of the top-k and bottom-k for\n",
    "        speed of convergennce of the pageranking components.\n",
    "        \"\"\"\n",
    "        if self.persist_topk is not None:\n",
    "            top_k = self.sorted_indices(ranking)[:self.persist_topk]\n",
    "            bottom_k = self.sorted_indices(ranking)[-self.persist_topk:]\n",
    "    \n",
    "            self.top_nodes.append(top_k)\n",
    "            self.bottom_nodes.append(bottom_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eae9c6b-940b-4558-8e7f-c4e867e619c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Linear System with Gauss-Seidel\n",
    "gauss_seidel_85 = GaussSeidelPageRank(alpha=0.85, tol=1e-8).fit(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249433d9-c174-4529-bf37-e765159b8b3b",
   "metadata": {},
   "source": [
    "### Helper class for presenting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fefce16-c274-45b3-b20b-81a43b0b8383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Helper class for presenting results\n",
    "\n",
    "class ResultsPresenter:\n",
    "    \"\"\"\n",
    "    A custom class for presenting results of pagerank methods\n",
    "    with respect to comparison.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, methods):\n",
    "        self.methods = methods\n",
    "\n",
    "    def rankings(self, topk = 20):\n",
    "        column_names = [method.identifier for method in self.methods]\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        for method in self.methods:\n",
    "            df[method.identifier] = method.sorted_indices()[:topk]\n",
    "\n",
    "        df.index = df.index+1\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def runtime_df(self):\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        method_names = []\n",
    "        method_alpha = []\n",
    "        method_iterations = []\n",
    "        method_runtime = []\n",
    "\n",
    "        for method in self.methods:\n",
    "            method_names.append(method.identifier)\n",
    "            method_alpha.append(method.alpha)\n",
    "            method_iterations.append(method.n_iterations)\n",
    "            method_runtime.append(method.runtime_ms)\n",
    "\n",
    "        df['Method'] = method_names\n",
    "        df['Iterations'] = method_iterations\n",
    "        df['Runtime (millis)'] = method_runtime\n",
    "\n",
    "        return df\n",
    "\n",
    "    def top_and_bottom_pages_convergence_plot(self):\n",
    "        fig, axes = plt.subplots(len(self.methods), 1, figsize=(12, 16))\n",
    "\n",
    "        for (method_idx, method) in enumerate(self.methods):\n",
    "            ax = axes[method_idx]\n",
    "\n",
    "            top_k_series = np.array(method.top_nodes).mean(axis=1).reshape(-1,1)\n",
    "            bottom_k_series = np.array(method.bottom_nodes).mean(axis=1).reshape(-1,1)\n",
    "\n",
    "            ax.plot(top_k_series, label='Top-k pages')\n",
    "            ax.plot(bottom_k_series, label='Bottom-k pages')\n",
    "\n",
    "            ax.legend()\n",
    "            title = \"Top-k and Bottom-K convergence speed.\\nMethod: {} for alpha={}\".format(\n",
    "                method.method_name, method.alpha)\n",
    "\n",
    "            ax.set_xlabel('Iterations')\n",
    "            ax.set_ylabel('Mean Convergence')\n",
    "            ax.set_title(title)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def error_history_plot(self):\n",
    "        for method in self.methods:\n",
    "            plt.plot(method.error_history, label=method.identifier)\n",
    "\n",
    "        plt.legend()\n",
    "        plt.title('Convergence error VS iterations')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef59b3-8236-4a53-b4fb-b0c8a0959f9e",
   "metadata": {},
   "source": [
    "### Remarks for 1.a\n",
    "\n",
    "Below you can find the results regarding:\n",
    "\n",
    "- whether or not the rankings between the 2 methods are the same or not\n",
    "- which method runs faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9fe321-0b94-41c2-b03f-54b57b4bffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ResultsPresenter([power_method_85, gauss_seidel_85])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7408b0-3181-4776-b713-409b17f50bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.rankings(topk=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dcfd7f-b04f-4713-b2d8-c13f4a84b605",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "Judging from the top-10 results it seems that running for a=0.85 and t=1e-8 the Power-Method and the Gauss-Seidel give **different** rankings.\\\n",
    "For example: In the first position Power-Method has put the webpage with id=89072 whereas the Gauss-Seidel has put the webpage with id=266297."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7c0c0c-9b46-4849-b3a9-4878ccfb55ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.runtime_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eec4c3b-1e28-4697-a825-b3422e82eabb",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "As it can be seen the power-method is much faster (around 10 times) than Gauss-Seidel.\\\n",
    "Just for fun, a graph can be found below which depicts the convergence error with respect to the number of iterations, for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b427b3-c5da-4207-bfa8-5dab2ce07474",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.error_history_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cb8c88-b399-4309-813d-30b5a34105ac",
   "metadata": {},
   "source": [
    "## b. Do the previous task with $\\alpha=0.99$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985b661e-1b47-47b4-9ff6-4247204447c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the power-method, for a=0.99\n",
    "power_method_99 = PowerMethodPageRank(alpha=0.99, tol=1e-8).fit(P)\n",
    "\n",
    "# Run the Linear System with Gauss-Seidel, or a=0.99\n",
    "gauss_seidel_99 = GaussSeidelPageRank(alpha=0.99, tol=1e-8).fit(P)\n",
    "\n",
    "# Construct a presenter for the results\n",
    "results = ResultsPresenter([power_method_99, gauss_seidel_99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd56ee0-b8a5-49e4-8cc4-7a0e3a63299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.rankings(topk=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ac7313-ba1f-4eab-82a2-140060b91fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the top-50 nodes ranking changed\n",
    "\n",
    "power_method_ranking_changed = not np.all(power_method_85.sorted_indices()[:50] == power_method_99.sorted_indices()[:50])\n",
    "gauss_seidel_ranking_changed = not np.all(gauss_seidel_85.sorted_indices()[:50] == gauss_seidel_99.sorted_indices()[:50])\n",
    "\n",
    "print(\"Power-Method top-50 ranking changed? ---> {}\".format(power_method_ranking_changed))\n",
    "print(\"Gauss-Seidel top-50 ranking changed? ---> {}\".format(gauss_seidel_ranking_changed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e0cc45-9ade-4c40-a4b5-6d7433324f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.runtime_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57e1521-502f-44bc-95a8-95a222db7426",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.error_history_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c086647-db3a-474f-b085-775d8b6d981f",
   "metadata": {},
   "source": [
    "### Remarks for 1.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9cfded-3427-4231-9791-f1e6cf7192b9",
   "metadata": {},
   "source": [
    "- Of course the running time increased (as expected due to increase of alpha) for both methods. Actually now, the power method is only around 3 times faster than Gauss-Seidel (not 10 times like before)\n",
    "\n",
    "- Running for a=0.99 resulted in the top-50 rankings to change, for both methods. For example:\n",
    "  - Power method: The 25th place was previously taken by the node 60209 but this place is now taken by 235495\n",
    "  - Gauss-Seidel: The 25th place was previously taken by the node 184658 but this place is now taken by 225365"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d15931a-9c27-4589-98ad-dd726335c88a",
   "metadata": {},
   "source": [
    "## c. When we use the power method do all the components of $\\pi$ converge at the same speed to their limits? Which of them converge faster? Does Gauss-Seidel behave the same way?\n",
    "\n",
    "In this section, we rerun the pagerank algorithm for both methods, using $\\alpha=0.85$, but this time we record the ranking of nodes on each iteration for the top 50 and bottom 50 nodes\\\n",
    "in order to analyze if some components of $\\pi$ converge at the same speed to their limits, with respect to both methods (aka Power-Method & Gauss-Seidel System Solution).\n",
    "\n",
    "Essentially we define a measure of fluctuation as the mean of the ids of the ranked pages. This allows to understand how on each iteration the ids change places.\\\n",
    "The idea is that if the top-k (or bottom-k) pages stop changing places then there is no fluctuation and we can visualize at around which iteration this change happens for a set of\n",
    "componenents of the $\\pi$ vector. It is important to note that the defined metric (aka the mean of node ids) has zero physical meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82aa02a-6296-4b19-b4d0-7897c6f40717",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_method_c = PowerMethodPageRank(alpha=0.85, tol=1e-8, persist_topk=50).fit(P)\n",
    "gauss_seidel_c = GaussSeidelPageRank(alpha=0.85, tol=1e-8, persist_topk=50).fit(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83400bc2-3c42-4e84-98d8-d2cf8c5be2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ResultsPresenter([power_method_c, gauss_seidel_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e56573c-0748-42ab-a447-c94d3385319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.top_and_bottom_pages_convergence_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa67934-0b0a-45c3-bab8-82ab4ef6236c",
   "metadata": {},
   "source": [
    "### Remarks for 1.c\n",
    "\n",
    "- For the Power-Method we observe that, the components that correspond to the important nodes (top-50) converge faster than those which correspond to non-important. More specifically for the top-50 nodes\n",
    "  this happens before the 20th iteration, while for the bottom-50 nodes this happens after the 40th iteration.\n",
    "- For the System-Formulation (with Gauss-Seidel) we observe again that the components that correspond to the important nodes converge much faster than those which correspond to non-important. More specifically,\n",
    "  for the top-50 nodes this happens before the 20th iteration. As opposed to the Power-Method, this time the bottom pages are fluctuating for a much longer time and they stop fluctuating around the end (about the 58th iteration.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677b7f3c-1f6d-4cf6-8264-147182381a38",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bd41cb-522c-4a71-bbc1-c6cc5daa21df",
   "metadata": {},
   "source": [
    "### a. Create a new web page X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d8dd98-5db7-4fd9-bf1c-6286984bb9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the original graph and add X as a web page\n",
    "data_with_X = data.copy()\n",
    "X_index = int(data_with_X.source.max()+1)\n",
    "data_with_X.loc[len(data_with_X.index)] = [X_index, X_index, 1.0]\n",
    "\n",
    "# define the new graph as matrix\n",
    "P_with_X = create_sparse_graph(data_with_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591ede6c-18fc-43ec-aac0-1ecd31bc05c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_with_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9434b1-e530-4536-8a45-1460ba1ace83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Power-Method Page-Rank (a=0.85)\n",
    "power_method_with_x = PowerMethodPageRank(alpha=0.85, tol=1e-8).fit(P_with_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e2a36a-5318-48d7-b4c9-b62891843a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_method_85.sorted_indices()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ea56e-97bc-458b-9d23-1ee60d0f1dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_method_with_x.sorted_indices()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca02eb-0db6-4c21-9287-e0530813e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets count the number of differences in the first 1000 positions\n",
    "old_rankings = power_method_85.sorted_indices()[:1000]\n",
    "rankings_with_x = power_method_with_x.sorted_indices()[:1000]\n",
    "\n",
    "np.sum(old_rankings != rankings_with_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629068b9-eeec-4296-b9d8-de8c0114ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the position (ranking) of the webpage\n",
    "# Note: +1 is needed because python counts from zero and we created the matrix by subtracting 1 from the source and target ids\n",
    "position_of_X=np.where((power_method_with_x.sorted_indices()+1) == X_index)[0][0]\n",
    "position_of_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8f833a-70a0-4d44-9fc3-b3fb3709d279",
   "metadata": {},
   "source": [
    "### Remarks for 2.a\n",
    "\n",
    "- After adding a new webpage X we observe that there are changes even in the top-50 nodes. For example after adding **X** the 29th position which was previously taken by node 247240\\\n",
    "is now taken by the node 259454 which before did not exist in the op-50 nodes. Also, for the first top-1000 positions we observe 142 differences in ranking positions.\n",
    "\n",
    "- The newly added webpage **X** can be found at the position 47896 - well this is dis-satisfying :-("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6355bf-6b8d-4f85-8ab5-c8d493c2579c",
   "metadata": {},
   "source": [
    "### b. Create another page Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b218725a-c1a2-4af1-951e-946a653338d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment the webpages by adding Y as a webpage as well\n",
    "Y_index = int(data_with_X.source.max() +1 )\n",
    "# Make Y to point to X\n",
    "data_with_X.loc[len(data_with_X.index)] = [Y_index, X_index, 1.0]\n",
    "\n",
    "# define the new graph as matrix\n",
    "P_with_XY = create_sparse_graph(data_with_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e7bd1d-579e-4920-b705-20e429a70749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun the algorithm and find the rankings of X and Y\n",
    "power_method_with_xy = PowerMethodPageRank(alpha=0.85, tol=1e-8).fit(P_with_XY)\n",
    "\n",
    "position_of_X=np.where((power_method_with_xy.sorted_indices()+1) == X_index)[0][0]\n",
    "position_of_Y=np.where((power_method_with_xy.sorted_indices()+1) == Y_index)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8643530d-6d4c-4a20-8b94-0113a3465d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PageRank of X: {}\".format(position_of_X))\n",
    "print(\"PageRank of Y: {}\".format(position_of_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ef80c-984f-40cd-8513-b5a3490dad77",
   "metadata": {},
   "source": [
    "### Remarks for 2.b\n",
    "\n",
    "Wow, even with one webpage pointing to X, X has a new improved ranking climbing to the position 16180! \\\n",
    "Observe that the rank of **Y** is even lower than the original ranking of **X**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d47f0f-006b-48ab-a8ad-438f51003ace",
   "metadata": {},
   "source": [
    "### c. Still unsatisfied, you create a third page Z\n",
    "\n",
    "Now intuitively, the best setup for the 3 pages in order to maximize the pagerank of X, is to put Y and Z both to point at X.\\\n",
    "This is because no one else in the graph points at X hence, the greater the number of in-links X has the more improved its pagerank will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596d4f53-a9ce-4138-abb7-e3514fea5c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the original graph and add X, Y and Z as web pages\n",
    "data_with_XYZ = data.copy()\n",
    "\n",
    "X_index = int(data_with_XYZ.source.max()+1)\n",
    "Y_index = int(data_with_XYZ.source.max()+2)\n",
    "Z_index = int(data_with_XYZ.source.max()+3)\n",
    "\n",
    "data_with_XYZ.loc[len(data_with_XYZ.index)] = [X_index, X_index, 1.0]\n",
    "data_with_XYZ.loc[len(data_with_XYZ.index)] = [Y_index, X_index, 1.0]\n",
    "data_with_XYZ.loc[len(data_with_XYZ.index)] = [Z_index, X_index, 1.0]\n",
    "\n",
    "# define the new graph as matrix\n",
    "P_with_XYZ = create_sparse_graph(data_with_XYZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b074f835-1ab9-4ac2-8b47-813bd3933f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun the algorithm and find the rankings of X , Y and Z\n",
    "power_method_with_xyz = PowerMethodPageRank(alpha=0.85, tol=1e-8).fit(P_with_XYZ)\n",
    "\n",
    "position_of_X=np.where((power_method_with_xyz.sorted_indices()+1) == X_index)[0][0]\n",
    "position_of_Y=np.where((power_method_with_xyz.sorted_indices()+1) == Y_index)[0][0]\n",
    "position_of_Z=np.where((power_method_with_xyz.sorted_indices()+1) == Z_index)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b9f173-38d2-4e6f-b472-06d0932b34f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PageRank of X: {}\".format(position_of_X))\n",
    "print(\"PageRank of Y: {}\".format(position_of_Y))\n",
    "print(\"PageRank of Z: {}\".format(position_of_Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040a4f9b-e1e6-4be6-ab46-e300ac16e222",
   "metadata": {},
   "source": [
    "### Remarks for 2.c\n",
    "\n",
    "The pageranking of **X** is improved even more (reached 11114)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d1250-be32-40ed-8e96-619dd2c8edc1",
   "metadata": {},
   "source": [
    "### d. Add links from your page X to older, popular pages. What happens to PageRank of X? You add links from Y or Z to older, popular pages. What happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c58331d-27ce-461d-a3f8-cd835fae4fb2",
   "metadata": {},
   "source": [
    "### d.1 Adding links from X to popular pages\n",
    "\n",
    "The top-20 pages are added as links from X (Y and Z still point to X). Lets see how the PageRank of X changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35218c0-71e7-4765-a67b-09c54ef8c00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_XYZ = data.copy()\n",
    "\n",
    "X_index = int(data_with_XYZ.source.max()+1)\n",
    "Y_index = int(data_with_XYZ.source.max()+2)\n",
    "Z_index = int(data_with_XYZ.source.max()+3)\n",
    "\n",
    "# Linking popular pages from X\n",
    "topk = 20\n",
    "top_pages = power_method_85.sorted_indices()[:topk]\n",
    "\n",
    "for top_page_id in top_pages:\n",
    "    data_with_XYZ.loc[len(data_with_XYZ.index)] = [X_index, top_page_id, 1.0/topk]\n",
    "\n",
    "# Y and Z point to X\n",
    "data_with_XYZ.loc[len(data_with_XYZ.index)] = [Y_index, X_index, 1.0]\n",
    "data_with_XYZ.loc[len(data_with_XYZ.index)] = [Z_index, X_index, 1.0]\n",
    "\n",
    "P_with_XYZ = create_sparse_graph(data_with_XYZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1c6386-512b-4c80-8eb3-cd77c47bddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun the algorithm and find the rankings of X , Y and Z\n",
    "power_method_with_xyz = PowerMethodPageRank(alpha=0.85, tol=1e-8).fit(P_with_XYZ)\n",
    "\n",
    "position_of_X=np.where((power_method_with_xyz.sorted_indices()+1) == X_index)[0][0]\n",
    "position_of_Y=np.where((power_method_with_xyz.sorted_indices()+1) == Y_index)[0][0]\n",
    "position_of_Z=np.where((power_method_with_xyz.sorted_indices()+1) == Z_index)[0][0]\n",
    "\n",
    "print(\"PageRank of X: {}\".format(position_of_X))\n",
    "print(\"PageRank of Y: {}\".format(position_of_Y))\n",
    "print(\"PageRank of Z: {}\".format(position_of_Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbd90f4-186e-42be-aa7b-a9c65bef5f7a",
   "metadata": {},
   "source": [
    "### Remarks for 2.d - 1\n",
    "\n",
    "The pagerank of X drops significantly, to position 109090. This is intuitive since the pagerank of the node is penalized because X points to the most\n",
    "popular pages, but only Y and Z point to X and these 2 pages are not outlinks of any node in the graph. This means that all the page rank that X had was distributed to\n",
    "the popular pages and in effect its position is demoted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c3bc7-4fe2-4267-acc9-158197265f6a",
   "metadata": {},
   "source": [
    "### d.2 Adding links from Y or Z to popular pages\n",
    "\n",
    "In this section both Y and Z point to popular pages. Specifically, Y will point to the first top-10 pages and Z to the second top-10 pages.\n",
    "Y and Z still will point to X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f462880d-08f4-444a-84af-7cb8186a31d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_XYZ = data.copy()\n",
    "\n",
    "X_index = int(data_with_XYZ.source.max()+1)\n",
    "Y_index = int(data_with_XYZ.source.max()+2)\n",
    "Z_index = int(data_with_XYZ.source.max()+3)\n",
    "\n",
    "# Add X\n",
    "data_with_XYZ.loc[len(data_with_XYZ.index)] = [X_index, X_index, 1.0]\n",
    "\n",
    "# Y and Z point to X\n",
    "data_with_XYZ.loc[len(data_with_XYZ.index)] = [Y_index, X_index, 1.0/10]\n",
    "data_with_XYZ.loc[len(data_with_XYZ.index)] = [Z_index, X_index, 1.0/10]\n",
    "\n",
    "# Linking popular pages from X\n",
    "topk = 20\n",
    "top_pages = power_method_85.sorted_indices()[:topk]\n",
    "\n",
    "for (rank_id, top_page_id) in enumerate(top_pages):\n",
    "    if rank_id <= 9:\n",
    "        data_with_XYZ.loc[len(data_with_XYZ.index)] = [Y_index, top_page_id, 1.0/10]\n",
    "    else:\n",
    "        data_with_XYZ.loc[len(data_with_XYZ.index)] = [Z_index, top_page_id, 1.0/10]\n",
    "\n",
    "P_with_XYZ = create_sparse_graph(data_with_XYZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d253c08e-d677-449b-a8b1-a978fa030e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun the algorithm and find the rankings of X , Y and Z\n",
    "power_method_with_xyz = PowerMethodPageRank(alpha=0.85, tol=1e-8).fit(P_with_XYZ)\n",
    "\n",
    "position_of_X=np.where((power_method_with_xyz.sorted_indices()+1) == X_index)[0][0]\n",
    "position_of_Y=np.where((power_method_with_xyz.sorted_indices()+1) == Y_index)[0][0]\n",
    "position_of_Z=np.where((power_method_with_xyz.sorted_indices()+1) == Z_index)[0][0]\n",
    "\n",
    "print(\"PageRank of X: {}\".format(position_of_X))\n",
    "print(\"PageRank of Y: {}\".format(position_of_Y))\n",
    "print(\"PageRank of Z: {}\".format(position_of_Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c65cf5-69a9-47b9-b585-f41a516307d6",
   "metadata": {},
   "source": [
    "### Remarks for 2.d-2\n",
    "\n",
    "The pagerank of **X** improved from the previous question reaching the position 26553. Still this position is worse than when Y and Z had X as their only outlink."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dd1ebc-9cdb-4f35-a101-4c7c9de5f7df",
   "metadata": {},
   "source": [
    "### e. Improve the PageRank of X further\n",
    "\n",
    "As it is evident from the previous sections, when Y and Z pointed to X then its pagerank improved dramatically (from 47896 to 11114). Thus it only makes sense that if some of the most important pages point to **X** then its pagerank would improve even further. So lets test this idea by putting the top-50 pages to point to **X**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9f4c4a-c04b-408b-a742-e3449a0ad707",
   "metadata": {},
   "outputs": [],
   "source": [
    "top30 = power_method_85.sorted_indices()[:30]\n",
    "\n",
    "# Make the top nodes to point to X\n",
    "for top_page_id in top30:\n",
    "    data_with_XYZ.loc[len(data_with_XYZ.index)] = [top_page_id, X_index, 1]\n",
    "    outlinks = len(data_with_XYZ[data_with_XYZ.source == top_page_id].target)\n",
    "    data_with_XYZ.loc[(data_with_XYZ.source == top_page_id), 'transition_probability'] = 1.0/outlinks\n",
    "\n",
    "P_linkfarm = create_sparse_graph(data_with_XYZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548803a9-3231-4eb8-bba1-3ce8fcd85744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun the algorithm and find the rankings of X , Y and Z\n",
    "power_method_linkfarm = PowerMethodPageRank(alpha=0.85, tol=1e-8).fit(P_linkfarm)\n",
    "\n",
    "position_of_X=np.where((power_method_linkfarm.sorted_indices()+1) == X_index)[0][0]\n",
    "position_of_Y=np.where((power_method_linkfarm.sorted_indices()+1) == Y_index)[0][0]\n",
    "position_of_Z=np.where((power_method_linkfarm.sorted_indices()+1) == Z_index)[0][0]\n",
    "\n",
    "print(\"PageRank of X: {}\".format(position_of_X))\n",
    "print(\"PageRank of Y: {}\".format(position_of_Y))\n",
    "print(\"PageRank of Z: {}\".format(position_of_Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0f8fe5-f140-49c2-9d47-a417b52adc41",
   "metadata": {},
   "source": [
    "### Remarks for 2.e\n",
    "\n",
    "We verified that if some of the most popular pages link to **X** then its pagerank is very much improved.\\\n",
    "Specifically putting the top-30 nodes to point at **X** it pageranking moved from the position 11114 to 1249 (which is a huge improvement)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
